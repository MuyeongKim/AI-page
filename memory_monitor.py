"""
###############################################################################################
# ğŸš€ AI-page í”„ë¡œì íŠ¸ìš© ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë„êµ¬ (2025.08.10)
###############################################################################################
# 
# ëª©ì : YOLO ê°ì²´íƒì§€ í”„ë¡œê·¸ë¨ì˜ ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
# ì‚¬ìš©ë²•: gui.pyì—ì„œ importí•˜ì—¬ MemoryMonitor í´ë˜ìŠ¤ í™œìš©
# 
# ì£¼ìš” ê¸°ëŠ¥:
# - ì‹¤ì‹œê°„ RAM/GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
# - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê²½ê³  ì‹œìŠ¤í…œ (2GB RAM, 4GB GPU ì´ˆê³¼ ì‹œ)
# - í”„ë¡œê·¸ë¨ ì‹¤í–‰ ìš”ì•½ ì •ë³´ ì œê³µ
# - ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë¡
#
# ì‘ì„±ì: Claude Code AI Assistant
# ë²„ì „: 1.0
###############################################################################################
"""

import torch
import psutil
import os
from datetime import datetime

class MemoryMonitor:
    """ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self):
        self.start_time = datetime.now()
        self.peak_gpu_memory = 0
        self.peak_ram_memory = 0
        
    def log_memory_usage(self, context=""):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        
        # GPU ë©”ëª¨ë¦¬ (CUDA ì‚¬ìš© ì‹œ)
        if torch.cuda.is_available():
            gpu_allocated = torch.cuda.memory_allocated() / 1024**2  # MB
            gpu_cached = torch.cuda.memory_reserved() / 1024**2
            self.peak_gpu_memory = max(self.peak_gpu_memory, gpu_allocated)
            gpu_info = f"GPU: {gpu_allocated:.1f}MB (ìºì‹œ: {gpu_cached:.1f}MB)"
        else:
            gpu_info = "GPU: ì‚¬ìš© ì•ˆí•¨"
            
        # RAM ë©”ëª¨ë¦¬
        process = psutil.Process(os.getpid())
        ram_mb = process.memory_info().rss / 1024**2
        self.peak_ram_memory = max(self.peak_ram_memory, ram_mb)
        
        print(f"[{timestamp}] {context} - RAM: {ram_mb:.1f}MB, {gpu_info}")
        
        # ë©”ëª¨ë¦¬ ê²½ê³ 
        if ram_mb > 2048:  # 2GB ì´ìƒ
            print("âš ï¸ RAM ì‚¬ìš©ëŸ‰ ë†’ìŒ!")
        if torch.cuda.is_available() and gpu_allocated > 4096:  # 4GB ì´ìƒ
            print("âš ï¸ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ!")
    
    def get_summary(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš© ìš”ì•½"""
        runtime = datetime.now() - self.start_time
        return (f"ì‹¤í–‰ì‹œê°„: {runtime.total_seconds():.1f}ì´ˆ, "
                f"ìµœëŒ€ RAM: {self.peak_ram_memory:.1f}MB, "
                f"ìµœëŒ€ GPU: {self.peak_gpu_memory:.1f}MB")

# gui.pyì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•:
"""
# gui.py ìƒë‹¨ì— ì¶”ê°€
from memory_monitor import MemoryMonitor

class Ui_MainWindow:
    def __init__(self):
        # ... ê¸°ì¡´ ì½”ë“œ ...
        self.memory_monitor = MemoryMonitor()
    
    def submit(self):
        self.memory_monitor.log_memory_usage("ëª¨ë¸ ë¡œë”© ì „")
        # ëª¨ë¸ ìƒì„± ì½”ë“œ...
        self.memory_monitor.log_memory_usage("ëª¨ë¸ ë¡œë”© í›„")
        
        # ì²˜ë¦¬ ë£¨í”„ì—ì„œ
        while True:
            # ... ì²˜ë¦¬ ì½”ë“œ ...
            if frame_counter % 100 == 0:
                self.memory_monitor.log_memory_usage(f"í”„ë ˆì„ {frame_counter}")
"""