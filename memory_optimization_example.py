###############################################################################################
# ğŸš€ AI-page ë©”ëª¨ë¦¬ ìµœì í™” íŒ¨í„´ ì˜ˆì‹œ ì½”ë“œ (2025.08.10)
###############################################################################################
#
# ëª©ì : YOLO ê°ì²´íƒì§€ í”„ë¡œê·¸ë¨ì„ ìœ„í•œ ê³ ê¸‰ ë©”ëª¨ë¦¬ ìµœì í™” íŒ¨í„´ ì œì‹œ
# ìƒíƒœ: í•™ìŠµìš© ì˜ˆì‹œ ì½”ë“œ (ì§ì ‘ ì‹¤í–‰ X, gui.py ê°œì„  ì‹œ ì°¸ê³ ìš©)
#
# í¬í•¨ëœ ìµœì í™” íŒ¨í„´:
# - Context Managerë¥¼ í™œìš©í•œ ìë™ ë©”ëª¨ë¦¬ ê´€ë¦¬
# - ë°°ì¹˜ ì²˜ë¦¬ìš© ë©”ëª¨ë¦¬ ìµœì í™” (BatchProcessor)
# - ì‹¤ì‹œê°„ ì²˜ë¦¬ ë£¨í”„ ìµœì í™”
# - dequeë¥¼ í™œìš©í•œ FPS ë²„í¼ ìµœì í™”
# - ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œìŠ¤í…œ
#
# ì‘ì„±ì: Claude Code AI Assistant
# ë²„ì „: 1.0
###############################################################################################

# ğŸš€ ê°œì„ ëœ ë©”ëª¨ë¦¬ ê´€ë¦¬ íŒ¨í„´ë“¤

import torch
import gc
import cv2
import numpy as np
from collections import deque
from contextlib import contextmanager

class OptimizedMemoryManager:
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ YOLO ì²˜ë¦¬ ë§¤ë‹ˆì €"""
    
    def __init__(self, buffer_size=30):
        # deque ì‚¬ìš©ìœ¼ë¡œ O(1) ì‹œê°„ë³µì¡ë„ í™•ë³´
        self.fps_buffer = deque(maxlen=buffer_size)
        self.frame_counter = 0
        self.cleanup_interval = 100  # 100í”„ë ˆì„ë§ˆë‹¤ ë©”ëª¨ë¦¬ ì •ë¦¬
        
    @contextmanager
    def yolo_inference(self, model, frame):
        """YOLO ì¶”ë¡  ê²°ê³¼ ìë™ ë©”ëª¨ë¦¬ ê´€ë¦¬"""
        results_list = None
        try:
            results_list = model(frame)
            yield results_list
        finally:
            # ì¶”ë¡  ê²°ê³¼ ì¦‰ì‹œ ì •ë¦¬
            if results_list:
                del results_list
            torch.cuda.empty_cache() if torch.cuda.is_available() else None

    @contextmanager 
    def frame_processing(self, cap):
        """í”„ë ˆì„ ì²˜ë¦¬ ìë™ ë©”ëª¨ë¦¬ ê´€ë¦¬"""
        frame = None
        try:
            ret, frame = cap.read()
            if ret:
                yield frame
        finally:
            # í”„ë ˆì„ ë©”ëª¨ë¦¬ ì¦‰ì‹œ í•´ì œ
            if frame is not None:
                del frame
            
    def periodic_cleanup(self):
        """ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬"""
        self.frame_counter += 1
        if self.frame_counter % self.cleanup_interval == 0:
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            print(f"ğŸ§¹ ì£¼ê¸°ì  ì •ë¦¬ ì™„ë£Œ (í”„ë ˆì„: {self.frame_counter})")

    def add_fps(self, fps_value):
        """FPS ë²„í¼ íš¨ìœ¨ì  ê´€ë¦¬"""
        self.fps_buffer.append(fps_value)
        return np.mean(self.fps_buffer)

# ğŸ¯ ì‚¬ìš© ì˜ˆì‹œ: ê°œì„ ëœ ì‹¤ì‹œê°„ ì²˜ë¦¬ ë£¨í”„
def optimized_realtime_processing(model, cap):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì‹¤ì‹œê°„ ì²˜ë¦¬"""
    memory_manager = OptimizedMemoryManager()
    
    try:
        while True:
            # ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¡œ í”„ë ˆì„ ìë™ ê´€ë¦¬
            with memory_manager.frame_processing(cap) as frame:
                if frame is None:
                    break
                    
                # ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¡œ ì¶”ë¡  ê²°ê³¼ ìë™ ê´€ë¦¬    
                with memory_manager.yolo_inference(model, frame) as results:
                    if len(results) > 0:
                        result = results[0]
                        
                        # ê²°ê³¼ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
                        process_detection_results(result, memory_manager)
                        
            # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬
            memory_manager.periodic_cleanup()
            
            # ESC í‚¤ë¡œ ì¢…ë£Œ
            if cv2.waitKey(1) & 0xFF == 27:
                break
                
    except Exception as e:
        print(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    finally:
        # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
        cleanup_all_resources()

def process_detection_results(result, memory_manager):
    """íƒì§€ ê²°ê³¼ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )"""
    # ì¦‰ì‹œ ì‚¬ìš©í•˜ê³  ë°”ë¡œ í•´ì œë˜ëŠ” ì„ì‹œ ë³€ìˆ˜ë“¤
    im_array = result.plot()
    
    try:
        # FPS ê³„ì‚°
        frame_time_ms = sum(result.speed.values())
        if frame_time_ms > 0:
            fps = 1000 / frame_time_ms
            avg_fps = memory_manager.add_fps(fps)
            
            # í™”ë©´ í‘œì‹œ (ë³µì‚¬ë³¸ ì‚¬ìš©)
            display_frame = im_array.copy()
            cv2.putText(display_frame, f"FPS: {avg_fps:.1f}", 
                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            cv2.imshow('Detections', display_frame)
            
    finally:
        # ì„ì‹œ ë³€ìˆ˜ë“¤ ì¦‰ì‹œ ì •ë¦¬
        del im_array
        if 'display_frame' in locals():
            del display_frame

def cleanup_all_resources():
    """ì „ì²´ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
    cv2.destroyAllWindows()
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    print("ğŸ§¹ ì „ì²´ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

# ğŸ”§ ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” (ëŒ€ìš©ëŸ‰ ì´ë¯¸ì§€ í´ë” ì²˜ë¦¬ìš©)
class BatchProcessor:
    """ë°°ì¹˜ ì²˜ë¦¬ ë©”ëª¨ë¦¬ ìµœì í™”"""
    
    def __init__(self, batch_size=8, cleanup_every=50):
        self.batch_size = batch_size
        self.cleanup_every = cleanup_every
        self.processed_count = 0
        
    def process_images_batch(self, model, image_paths):
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°°ì¹˜ ì²˜ë¦¬"""
        total_images = len(image_paths)
        
        for i in range(0, total_images, self.batch_size):
            batch = image_paths[i:i + self.batch_size]
            
            # ë°°ì¹˜ ì²˜ë¦¬
            with self.batch_context():
                self.process_single_batch(model, batch)
            
            # ì£¼ê¸°ì  ì •ë¦¬
            self.processed_count += len(batch)
            if self.processed_count % self.cleanup_every == 0:
                self.deep_cleanup()
                print(f"ğŸ§¹ ë°°ì¹˜ ì •ë¦¬: {self.processed_count}/{total_images}")
    
    @contextmanager
    def batch_context(self):
        """ë°°ì¹˜ ì²˜ë¦¬ ì»¨í…ìŠ¤íŠ¸"""
        try:
            yield
        finally:
            # ë°°ì¹˜ ì™„ë£Œ í›„ ì •ë¦¬
            gc.collect()
    
    def process_single_batch(self, model, image_paths):
        """ë‹¨ì¼ ë°°ì¹˜ ì²˜ë¦¬"""
        for image_path in image_paths:
            try:
                # ì´ë¯¸ì§€ ë¡œë“œ ë° ì²˜ë¦¬
                results = model(image_path)
                # ê²°ê³¼ ì²˜ë¦¬...
                
            finally:
                # ì¦‰ì‹œ ì •ë¦¬
                if 'results' in locals():
                    del results
    
    def deep_cleanup(self):
        """ê¹Šì€ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()  # CUDA ì‘ì—… ì™„ë£Œ ëŒ€ê¸°

# ğŸ¥ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë„êµ¬
class MemoryMonitor:
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
    
    @staticmethod
    def get_memory_info():
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**2  # MB
            cached = torch.cuda.memory_reserved() / 1024**2      # MB
            return f"CUDA ë©”ëª¨ë¦¬ - í• ë‹¹: {allocated:.1f}MB, ìºì‹œ: {cached:.1f}MB"
        return "CPU ë©”ëª¨ë¦¬ ëª¨ë“œ"
    
    @staticmethod
    def memory_alert(threshold_mb=1024):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê²½ê³ """
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**2
            if allocated > threshold_mb:
                print(f"âš ï¸ ë©”ëª¨ë¦¬ ê²½ê³ : {allocated:.1f}MB ì‚¬ìš© ì¤‘")
                return True
        return False

# ì‚¬ìš© ì˜ˆì‹œ:
if __name__ == "__main__":
    print("ë©”ëª¨ë¦¬ ìµœì í™” íŒ¨í„´ ì˜ˆì‹œ ì½”ë“œ")
    print("ì‹¤ì œ gui.pyì— ì ìš©í•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.")